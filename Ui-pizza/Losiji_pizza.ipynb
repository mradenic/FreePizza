{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.8"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Dora Raštegorac\n\nPodaci koje gledamo su zadani u train.json i test.json datotekama tako da prvo moramo napraviti import json.\n\nJSON datoteka je datoteka koja pohranjuje jednostavne strukture podataka i objekte u formatu JavaScript Object Notation, koji je standardni format za razmjenu podataka. Najviše se koristi za prijenos podataka između web-aplikacije i poslužitelja. One su temeljene na teksu, lako čitljive i mogu se uređivati pomoću uređivača teksta. ","metadata":{}},{"cell_type":"code","source":"import json\n\nwith open('train.json') as fin:\n    train_json = json.load(fin)\nprint(\"\\n\\n\")\nprint(\"----------------- train.json -------------------\\n\\n\")\n\nprint(\"Ispisujemo train_json[0] po elementima:\\n\")\n#gledamo sadrzaj prvog \"elementa\" train_json\nprint('UID:\\t', train_json[0]['request_id'], '\\n')\nprint('Title:\\t', train_json[0]['request_title'], '\\n')\nprint('Text:\\t', train_json[0]['request_text_edit_aware'], '\\n')\nprint('Tag:\\t', train_json[0]['requester_received_pizza'], end='\\n')\nprint('\\n\\n')","metadata":{},"execution_count":1,"outputs":[{"name":"stdout","text":"\n\n\n----------------- train.json -------------------\n\n\nIspisujemo train_json[0] po elementima:\n\nUID:\t t3_l25d7 \n\nTitle:\t Request Colorado Springs Help Us Please \n\nText:\t Hi I am in need of food for my 4 children we are a military family that has really hit hard times and we have exahusted all means of help just to be able to feed my family and make it through another night is all i ask i know our blessing is coming so whatever u can find in your heart to give is greatly appreciated \n\nTag:\t False\n\n\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Kako bi vidjeli primjer izgleda nama bitnih podataka ispisali smo te podatke za prvi element train.json datoteke, a sada ćemo napraviti konverziju 'json -> pandas dataframe' te u tablici\nispisati nama bitne podatke za prvih 10 zahtjeva ","metadata":{}},{"cell_type":"code","source":"print(\"\\n\\n ispisujemo prvih 10 zahtjeva danih u train.json:\\n\")\nimport pandas as pd\ndf = pd.json_normalize(train_json) \ndf_train_json = df[['request_id', 'request_title', \n               'request_text_edit_aware', \n               'requester_received_pizza']]\nprint(df_train_json.head(10))\nprint('\\n\\n')","metadata":{},"execution_count":2,"outputs":[{"name":"stdout","text":"\n\n ispisujemo prvih 10 zahtjeva danih u train.json:\n\n  request_id                                      request_title  \\\n0   t3_l25d7            Request Colorado Springs Help Us Please   \n1   t3_rcb83  [Request] California, No cash and I could use ...   \n2   t3_lpu5j  [Request] Hungry couple in Dundee, Scotland wo...   \n3   t3_mxvj3  [Request] In Canada (Ontario), just got home f...   \n4  t3_1i6486  [Request] Old friend coming to visit. Would LO...   \n5  t3_14gmeb  [REQUEST] I'll give a two week xbox live code ...   \n6   t3_wcw5m  [Request] Help me give back to my roomies on F...   \n7   t3_of16d  random acts of pizza, i have a request, if not...   \n8  t3_1ioo1k  [Request] Queensland Australia, Recently moved...   \n9   t3_k0l9j          [REQUEST]We're in need of some om noms...   \n\n                             request_text_edit_aware  requester_received_pizza  \n0  Hi I am in need of food for my 4 children we a...                     False  \n1  I spent the last money I had on gas today. Im ...                     False  \n2  My girlfriend decided it would be a good idea ...                     False  \n3  It's cold, I'n hungry, and to be completely ho...                     False  \n4  hey guys:\\n I love this sub. I think it's grea...                     False  \n5  Feeling under the weather so I called out off ...                      True  \n6  We're in Tampa Florida...moving to Ybor on Fri...                     False  \n7  (Request) I have given a few things on reddit,...                     False  \n8  Wasnt really sure what to put as the title, un...                     False  \n9  Austin, Texas\\n\\nMy two roommates and I are hu...                      True  \n\n\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"U kodu ćemo to isto napraviti i za test.json datoteku, no uočimo da nam requester_received_pizza nije poznat - to mi trebamo odrediti","metadata":{}},{"cell_type":"code","source":"with open('test.json') as fin:\n    test_json = json.load(fin)\n\t\nprint(\"\\n\")\nprint(\"--------------------- test.json ------------------------\\n\\n\")\nprint(\"Ispisujemo test_json[0] po elementima:\\n\")\t\n\nprint('UID:\\t', test_json[0]['request_id'], '\\n')\nprint('Title:\\t', test_json[0]['request_title'], '\\n')\nprint('Text:\\t', test_json[0]['request_text_edit_aware'], '\\n')\n\n#pretvaramo test_json u dataframe\nprint(\"\\n\\n ispisujemo prvih 10 zahtjeva danih u test.json:\\n\\n\")\ndf = pd.json_normalize(test_json) # Pandas magic... \ndf_test_json = df[['request_id', 'request_title', \n               'request_text_edit_aware']]\nprint(df_test_json.head(10))\nprint('\\n\\n')","metadata":{},"execution_count":3,"outputs":[{"name":"stdout","text":"\n\n--------------------- test.json ------------------------\n\n\nIspisujemo test_json[0] po elementima:\n\nUID:\t t3_i8iy4 \n\nTitle:\t [request] pregger gf 95 degree house and no food.. promise to pay it forward! Northern Colorado \n\nText:\t Hey all! It's about 95 degrees here and our kitchen is pretty much empty save for some bread and cereal.  My girlfriend/fiance is 8 1/2 months pregnant and we could use a good meal.  We promise to pay it forward when we get money! Thanks so much in advance! \n\n\n\n ispisujemo prvih 10 zahtjeva danih u test.json:\n\n\n  request_id                                      request_title  \\\n0   t3_i8iy4  [request] pregger gf 95 degree house and no fo...   \n1  t3_1mfqi0  [Request] Lost my job day after labour day, st...   \n2   t3_lclka                (Request) pizza for my kids please?   \n3  t3_1jdgdj  [Request] Just moved to a new state(Waltham MA...   \n4   t3_t2qt4  [Request] Two girls in between paychecks, we'v...   \n5   t3_pvojb           [REQUEST] It's my birthday tomorrow (UK)   \n6  t3_142n4c  [Request] Just kindof sad/disappointed, could ...   \n7  t3_17rja6    [Request] Visiting student could use warm food.   \n8  t3_1lg6u2  [Request] Pregnant, packing @ 2am to move tomo...   \n9  t3_1b0mtx  [Request] My partner and I hit six-months, we ...   \n\n                             request_text_edit_aware  \n0  Hey all! It's about 95 degrees here and our ki...  \n1  I didn't know a place like this exists! \\n\\nI ...  \n2  Hi Reddit. Im a single dad having a really rou...  \n3  Hi I just moved to Waltham MA from my home sta...  \n4  We're just sitting here near indianapolis on o...  \n5  So, I'm a student in London, and it's my birth...  \n6  I'm not entirely sure why, I guess just kindof...  \n7  I'm a visiting medical student from Costa Rica...  \n8  My SO and I are moving to the new apartment to...  \n9  My partner is a wonderful gender-queer pansexu...  \n\n\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Stopwords u NLTK-u su engleske riječi koje ne daju skoro nikakvo značenje rečenici te ignorirajući ih ne žrtvujemo značenje rečenice. Njih učitavamo s naredbom 'from nltk.corpus import stopwords'.\n\nTakođer smijemo ignorirati interpunkcijske znakove, njih učitavamo naredbom 'from string import punctuation'.\n\nUočimo da u rečenicama još postoje npr. \"modal verbs\", tj. riječi tipa: can, could, should itd. Njih također želimo maknuti iz naših rečenica, no one nisu uključene u nltk.corpus pa ćemo ih unjeti sami.\n\nSve ćemo ih spremiti u poseban set i na kraju napraviti njihovu uniju.","metadata":{}},{"cell_type":"code","source":"from nltk.corpus import stopwords\n\nstopwords_en = stopwords.words('english')\n\nstopwords_en = set(stopwords.words('english'))\n\nfrom string import punctuation\n\nstopwords_en_withpunct = stopwords_en.union(set(punctuation))\n\nstopwords_json = {\"en\":[\"a\",\"a's\",\"able\",\"about\",\"above\",\"according\",\"accordingly\",\n                  \"across\",\"actually\",\"after\",\"afterwards\",\"again\",\"against\",\"ain't\",\n\t\t\t\t  \"all\",\"allow\",\"allows\",\"almost\",\"alone\",\"along\",\"already\",\"also\",\n\t\t\t\t  \"although\",\"always\",\"am\",\"among\",\"amongst\",\"an\",\"and\",\"another\",\n\t\t\t\t  \"any\",\"anybody\",\"anyhow\",\"anyone\",\"anything\",\"anyway\",\"anyways\",\n\t\t\t\t  \"anywhere\",\"apart\",\"appear\",\"appreciate\",\"appropriate\",\"are\",\n\t\t\t\t  \"aren't\",\"around\",\"as\",\"aside\",\"ask\",\"asking\",\"associated\",\"at\",\n\t\t\t\t  \"available\",\"away\",\"awfully\",\"b\",\"be\",\"became\",\"because\",\"become\",\n\t\t\t\t  \"becomes\",\"becoming\",\"been\",\"before\",\"beforehand\",\"behind\",\"being\",\n\t\t\t\t  \"believe\",\"below\",\"beside\",\"besides\",\"best\",\"better\",\"between\",\n\t\t\t\t  \"beyond\",\"both\",\"brief\",\"but\",\"by\",\"c\",\"c'mon\",\"c's\",\"came\",\"can\",\n\t\t\t\t  \"can't\",\"cannot\",\"cant\",\"cause\",\"causes\",\"certain\",\"certainly\",\n\t\t\t\t  \"changes\",\"clearly\",\"co\",\"com\",\"come\",\"comes\",\"concerning\",\"consequently\",\n\t\t\t\t  \"consider\",\"considering\",\"contain\",\"containing\",\"contains\",\"corresponding\",\n\t\t\t\t  \"could\",\"couldn't\",\"course\",\"currently\",\"d\",\"definitely\",\"described\",\n\t\t\t\t  \"despite\",\"did\",\"didn't\",\"different\",\"do\",\"does\",\"doesn't\",\"doing\",\n\t\t\t\t  \"don't\",\"done\",\"down\",\"downwards\",\"during\",\"e\",\"each\",\"edu\",\"eg\",\n\t\t\t\t  \"eight\",\"either\",\"else\",\"elsewhere\",\"enough\",\"entirely\",\"especially\",\n\t\t\t\t  \"et\",\"etc\",\"even\",\"ever\",\"every\",\"everybody\",\"everyone\",\"everything\",\n\t\t\t\t  \"everywhere\",\"ex\",\"exactly\",\"example\",\"except\",\"f\",\"far\",\"few\",\"fifth\",\n\t\t\t\t  \"first\",\"five\",\"followed\",\"following\",\"follows\",\"for\",\"former\",\n\t\t\t\t  \"formerly\",\"forth\",\"four\",\"from\",\"further\",\"furthermore\",\"g\",\"get\",\n\t\t\t\t  \"gets\",\"getting\",\"given\",\"gives\",\"go\",\"goes\",\"going\",\"gone\",\"got\",\n\t\t\t\t  \"gotten\",\"greetings\",\"h\",\"had\",\"hadn't\",\"happens\",\"hardly\",\"has\",\n\t\t\t\t  \"hasn't\",\"have\",\"haven't\",\"having\",\"he\",\"he's\",\"hello\",\"help\",\n\t\t\t\t  \"hence\",\"her\",\"here\",\"here's\",\"hereafter\",\"hereby\",\"herein\",\"hereupon\",\n\t\t\t\t  \"hers\",\"herself\",\"hi\",\"him\",\"himself\",\"his\",\"hither\",\"hopefully\",\"how\",\n\t\t\t\t  \"howbeit\",\"however\",\"i\",\"i'd\",\"i'll\",\"i'm\",\"i've\",\"ie\",\"if\",\"ignored\",\n\t\t\t\t  \"immediate\",\"in\",\"inasmuch\",\"inc\",\"indeed\",\"indicate\",\"indicated\",\n\t\t\t\t  \"indicates\",\"inner\",\"insofar\",\"instead\",\"into\",\"inward\",\"is\",\"isn't\",\n\t\t\t\t  \"it\",\"it'd\",\"it'll\",\"it's\",\"its\",\"itself\",\"j\",\"just\",\"k\",\"keep\",\n\t\t\t\t  \"keeps\",\"kept\",\"know\",\"known\",\"knows\",\"l\",\"last\",\"lately\",\"later\",\n\t\t\t\t  \"latter\",\"latterly\",\"least\",\"less\",\"lest\",\"let\",\"let's\",\"like\",\n\t\t\t\t  \"liked\",\"likely\",\"little\",\"look\",\"looking\",\"looks\",\"ltd\",\"m\",\"mainly\",\n\t\t\t\t  \"many\",\"may\",\"maybe\",\"me\",\"mean\",\"meanwhile\",\"merely\",\"might\",\"more\",\n\t\t\t\t  \"moreover\",\"most\",\"mostly\",\"much\",\"must\",\"my\",\"myself\",\"n\",\"name\",\n\t\t\t\t  \"namely\",\"nd\",\"near\",\"nearly\",\"necessary\",\"need\",\"needs\",\"neither\",\n\t\t\t\t  \"never\",\"nevertheless\",\"new\",\"next\",\"nine\",\"no\",\"nobody\",\"non\",\n\t\t\t\t  \"none\",\"noone\",\"nor\",\"normally\",\"not\",\"nothing\",\"novel\",\"now\",\n\t\t\t\t  \"nowhere\",\"o\",\"obviously\",\"of\",\"off\",\"often\",\"oh\",\"ok\",\"okay\",\n\t\t\t\t  \"old\",\"on\",\"once\",\"one\",\"ones\",\"only\",\"onto\",\"or\",\"other\",\"others\",\n\t\t\t\t  \"otherwise\",\"ought\",\"our\",\"ours\",\"ourselves\",\"out\",\"outside\",\"over\",\n\t\t\t\t  \"overall\",\"own\",\"p\",\"particular\",\"particularly\",\"per\",\"perhaps\",\n\t\t\t\t  \"placed\",\"please\",\"plus\",\"possible\",\"presumably\",\"probably\",\"provides\",\n\t\t\t\t  \"q\",\"que\",\"quite\",\"qv\",\"r\",\"rather\",\"rd\",\"re\",\"really\",\"reasonably\",\n\t\t\t\t  \"regarding\",\"regardless\",\"regards\",\"relatively\",\"respectively\",\"right\",\n\t\t\t\t  \"s\",\"said\",\"same\",\"saw\",\"say\",\"saying\",\"says\",\"second\",\"secondly\",\n\t\t\t\t  \"see\",\"seeing\",\"seem\",\"seemed\",\"seeming\",\"seems\",\"seen\",\"self\",\n\t\t\t\t  \"selves\",\"sensible\",\"sent\",\"serious\",\"seriously\",\"seven\",\"several\",\n\t\t\t\t  \"shall\",\"she\",\"should\",\"shouldn't\",\"since\",\"six\",\"so\",\"some\",\"somebody\",\n\t\t\t\t  \"somehow\",\"someone\",\"something\",\"sometime\",\"sometimes\",\"somewhat\",\n\t\t\t\t  \"somewhere\",\"soon\",\"sorry\",\"specified\",\"specify\",\"specifying\",\"still\",\n\t\t\t\t  \"sub\",\"such\",\"sup\",\"sure\",\"t\",\"t's\",\"take\",\"taken\",\"tell\",\"tends\",\n\t\t\t\t  \"th\",\"than\",\"thank\",\"thanks\",\"thanx\",\"that\",\"that's\",\"thats\",\"the\",\n\t\t\t\t  \"their\",\"theirs\",\"them\",\"themselves\",\"then\",\"thence\",\"there\",\"there's\",\n\t\t\t\t  \"thereafter\",\"thereby\",\"therefore\",\"therein\",\"theres\",\"thereupon\",\n\t\t\t\t  \"these\",\"they\",\"they'd\",\"they'll\",\"they're\",\"they've\",\"think\",\"third\",\n\t\t\t\t  \"this\",\"thorough\",\"thoroughly\",\"those\",\"though\",\"three\",\"through\",\n\t\t\t\t  \"throughout\",\"thru\",\"thus\",\"to\",\"together\",\"too\",\"took\",\"toward\",\"towards\",\n\t\t\t\t  \"tried\",\"tries\",\"truly\",\"try\",\"trying\",\"twice\",\"two\",\"u\",\"un\",\"under\",\n\t\t\t\t  \"unfortunately\",\"unless\",\"unlikely\",\"until\",\"unto\",\"up\",\"upon\",\"us\",\n\t\t\t\t  \"use\",\"used\",\"useful\",\"uses\",\"using\",\"usually\",\"uucp\",\"v\",\"value\",\n\t\t\t\t  \"various\",\"very\",\"via\",\"viz\",\"vs\",\"w\",\"want\",\"wants\",\"was\",\"wasn't\",\n\t\t\t\t  \"way\",\"we\",\"we'd\",\"we'll\",\"we're\",\"we've\",\"welcome\",\"well\",\"went\",\n\t\t\t\t  \"were\",\"weren't\",\"what\",\"what's\",\"whatever\",\"when\",\"whence\",\"whenever\",\n\t\t\t\t  \"where\",\"where's\",\"whereafter\",\"whereas\",\"whereby\",\"wherein\",\"whereupon\",\n\t\t\t\t  \"wherever\",\"whether\",\"which\",\"while\",\"whither\",\"who\",\"who's\",\"whoever\",\n\t\t\t\t  \"whole\",\"whom\",\"whose\",\"why\",\"will\",\"willing\",\"wish\",\"with\",\"within\",\n\t\t\t\t  \"without\",\"won't\",\"wonder\",\"would\",\"wouldn't\",\"x\",\"y\",\"yes\",\"yet\",\n\t\t\t\t  \"you\",\"you'd\",\"you'll\",\"you're\",\"you've\",\"your\",\"yours\",\"yourself\",\n\t\t\t\t  \"yourselves\",\"z\",\"zero\"]}\nstopwords_json_en = set(stopwords_json['en'])\nstopwords_nltk_en = set(stopwords.words('english'))\nstopwords_punct = set(punctuation)\n\nstoplist_combined = set.union(stopwords_json_en, stopwords_nltk_en, stopwords_punct)\nprint('\\n')","metadata":{},"execution_count":4,"outputs":[{"name":"stdout","text":"\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Sentence tokenization - proces razdvajanja stringova u rečenice ('from nltk import sent_tokenize') - u NLTK-u sent_tokenize()  je defaultna funkcija kojom rastavljamo stringove u rečenice\n\nWord tokenization - proces razdvajanja rečenica u riječi ('from nltk import word_tokenize') - u NLTK-u word_tokenize() je defaultna funkcija kojom rastavljamo rečenice u riječi\n\nZa demonstraciju ćemo ispisati request_text_edit_aware iz prvog zahtjeva train.json datoteke iz kojeg ćemo isključiti sve gore navedene stopwords (ovaj dio se ne nalazi u kodu-služi za objašnjenje)","metadata":{}},{"cell_type":"code","source":"from nltk import sent_tokenize, word_tokenize\n\ntrain1_lowered = list(map(str.lower, word_tokenize(train_json[0]['request_text_edit_aware'])))\nprint([word for word in train1_lowered if word not in stoplist_combined])\nprint('\\n')","metadata":{},"execution_count":5,"outputs":[{"name":"stdout","text":"['food', '4', 'children', 'military', 'family', 'hit', 'hard', 'times', 'exahusted', 'means', 'feed', 'family', 'make', 'night', 'blessing', 'coming', 'find', 'heart', 'give', 'greatly', 'appreciated']\n\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Primjetimo da ćemo gornjim postupkom dobiti puno riječi koje su načinjene od korijena riječi i nekog nastavka. Nama će u daljnjem postupku trebati samo korijen te riječi.\n\nZa to koristimo procese stemming-a i lematizacije. Stemming pokušava skratiti riječ koristeći standardna regex pravila, a lematizacija pokušava naći korijen riječi uz pomoć lingvističkih pravila (uz korištenje regex-a).\n\nZa stemming ćemo koristiti Porter Stemmer iz Porter(1980), a za lematizaciju Wordnet Lemmatizer","metadata":{}},{"cell_type":"markdown","source":"Po defaultu WordNetLemmatizer.lemmatize() funkcija će pretpostaviti, ako u inputu nema eksplicitne POS oznake, da je riječ imenica. \nIz gore navedenog slijedi da ćemo prvo trebati primjeniti pos_tag funkciju. Ona će obaviti tagging proces, tj. označiti će riječ u rečenici s pripadajućom oznakom, u ovisnosti o njenom kontekstu i definiciji. \n\nNakon toga tu rečenicu možemo obraditi u WordNet Lemmatizer-u. Uočimo: proces lematizacije neće funkcionirati na samo jednoj riječi bez njenog konteksta i pripadne POS oznake, bitno je da znamo je li ta riječ imenica ili pridjev itd.","metadata":{}},{"cell_type":"code","source":"from nltk.stem import PorterStemmer\nporter = PorterStemmer()\n\nfrom nltk.stem import WordNetLemmatizer\nwnl = WordNetLemmatizer()\n\nfrom nltk import pos_tag\n\ndef f1(ptag):\n    \n    mtag = {'NN':'n', 'JJ':'a',\n                  'VB':'v', 'RB':'r'}\n    try:\n        return mtag[ptag[:2]]\n    except:\n        return 'n'\n    \ndef f2(text):   \n    return [wnl.lemmatize(word.lower(), pos=f1(tag)) \n            for word, tag in pos_tag(word_tokenize(text))]   \n\n\ndef preprocess_text(text):\n    return [word for word in f2(text) \n            if word not in stoplist_combined\n            and not word.isdigit()]","metadata":{},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"Gore implementirana funkcija f1 konvertira Penn Treebank oznake u WordNet, a funkcija f2 izvodi lematizaciju.\n\nZa demonstraciju ćemo ispisati request_text_edit_aware iz prvog zahtjeva train.json datoteke iz kojeg ćemo isključiti sve gore navedene stopwords i obaviti proces lematizacije.","metadata":{}},{"cell_type":"code","source":"print([word for word in f2(train_json[0]['request_text_edit_aware']) \n       if word not in stoplist_combined\n       and not word.isdigit() ])\nprint('\\n')","metadata":{},"execution_count":7,"outputs":[{"name":"stdout","text":"['food', 'child', 'military', 'family', 'hit', 'hard', 'time', 'exahusted', 'fee', 'family', 'make', 'night', 'blessing', 'find', 'heart', 'give', 'greatly', 'appreciated']\n\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Što je vektorizacija? Neka su a i b rečenice te c vektor stringova koji sadrži sve korijene riječi iz obje rečenice (koje se ne nalaze u našem polju stoplist_combined). Prikažemo vektor a1, pripadne rečenice a, koji će na prvom mjestu imati upisan broj pojavljivanja prve riječi vektora c, na drugom mjestu broj pojavljivanja druge riječi vektora c itd. Analogno vrijedi i za rečenicu b.\n\nU scikit-learnu postoji već ugrađena funkcija koja obavlja preprocesiranje i vektorizaciju - to je CountVectorizer, tj. ta funkcija miče interpunkcijske znakove i mijenja svim riječima prvo slovo u malo slovo. No, njen analyzer možemo \"nadglasati\" našim preprocess_text-om tako da kao ulazni argument gornje funkcije satvimo analyzer=preprocess_text.\n\nTe ako želimo da nam se povratna informacija, tj. rješenje vektorizacije vrati u obliku matrice koristimo funkciju CountVectorizer.transform() ","metadata":{}},{"cell_type":"code","source":"from collections import Counter\n\nfrom io import StringIO\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nfrom operator import itemgetter\n","metadata":{},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"U nastavku koristimo naivnog Bayesa\n\nKlasifikacija - stavimo naše podatke u \"kutije\", tj. dodajemo oznake našim podacima, npr. imamo kutiju povrća i sortiramo ih u mrkve, brokulu itd.","metadata":{}},{"cell_type":"markdown","source":"Prvo ćemo podijeliti podatke za trening u dva dijela: training (koristimo za \"treniranje naših modela\") i validation (koristimo ih da provjerimo \"zvučanje\" našeg modela, tj. provjeravamo valja li)\n\ntrain_test_split dijeli naše podatke na dva dijela s obzirom na test-veličinu koju ćemo postaviti kao argument","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split \n\ntrain, valid = train_test_split(df_train_json, test_size=0.2)","metadata":{},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"Inicijaliziramo vektorizaciju i \"nadglasamo\" analyzer s našim preprocess_text","metadata":{}},{"cell_type":"code","source":"count_vect = CountVectorizer(analyzer=preprocess_text)\n\ntrain_set = count_vect.fit_transform(train['request_text_edit_aware'])\ntrain_tags = train['requester_received_pizza']\n\nvalid_set = count_vect.transform(valid['request_text_edit_aware'])\nvalid_tags = valid['requester_received_pizza']\n\ntest_set = count_vect.transform(df_test_json['request_text_edit_aware'])","metadata":{},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"Postoji mnogo varijanti Naivnog Bayesovog klasifikatora u sklearnu, a mi ćemo koristiti MultinomialNB.","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nclf = MultinomialNB()\n\nclf.fit(train_set, train_tags)  #ovom naredbom treniramo klasifikator","metadata":{},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"},"metadata":{}}]},{"cell_type":"markdown","source":"Prije nego što ćemo koristiti naš klasifikator na traženom testnom skupu želimo dobiti dojam kako radi, tj. koliko je dobar pa ćemo ga isprobati na podacima iz valid_set","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\npredictions_valid = clf.predict(valid_set)\n\nprint('Pizza reception accuracy = {}'.format(\n        accuracy_score(predictions_valid, valid_tags) * 100)\n     )","metadata":{},"execution_count":12,"outputs":[{"name":"stdout","text":"Pizza reception accuracy = 74.75247524752476\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Sada ćemo koristiti podatke iz cijelog df_train, tj. iz originalnog zadanog train skupa i ponovno ćemo vektorizirati i trenirati naš klasifikator\n\nZašto? U većini slučajeva treniranje na više podataka znači i bolji model","metadata":{}},{"cell_type":"code","source":"count_vect = CountVectorizer(analyzer=preprocess_text)\n\nfull_train_set = count_vect.fit_transform(df_train_json['request_text_edit_aware'])\nfull_tags = df_train_json['requester_received_pizza']\n\ntest_set = count_vect.transform(df_test_json['request_text_edit_aware'])\n\nclf = MultinomialNB() \nclf.fit(full_train_set, full_tags)","metadata":{},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"},"metadata":{}}]},{"cell_type":"markdown","source":"Za kraj napravimo predviđanje za cijeli skup test koji nam je bio zadan na početku -> koristimo .predict()","metadata":{}},{"cell_type":"code","source":"predictions = clf.predict(test_set)\n\nsuccess_rate = sum(df_train_json['requester_received_pizza']) / len(df_train_json) * 100\nprint(str('Of {} requests, only {} gets their pizzas,'\n          ' {}% success rate...'.format(len(df_train_json), \n                                        sum(df_train_json['requester_received_pizza']), \n                                       success_rate)\n         )\n     )\n\t \nsuccess_rate = sum(predictions) / len(predictions) * 100\nprint(\"\\n\")\nprint(str('Of {} requests, only {} gets their pizzas,'\n          ' {}% success rate...'.format(len(predictions), \n                                        sum(predictions), \n                                       success_rate)\n         )\n     )\n\t \nprint(\"\\n\")","metadata":{},"execution_count":14,"outputs":[{"name":"stdout","text":"Of 4040 requests, only 994 gets their pizzas, 24.603960396039604% success rate...\n\n\nOf 1631 requests, only 56 gets their pizzas, 3.4334763948497855% success rate...\n\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Da bi mogli provjeriti koliku točnost ima naš algoritam rješenja moramo zapisati u datoteci oblika .csv","metadata":{}},{"cell_type":"code","source":"df_output = pd.DataFrame({'request_id': list(df_test_json['request_id']),   #spajamo stupac request_id s našim predviđanjima\n                          'requester_received_pizza': list(predictions)}\n                        )\n\nprint(\"requester_received_pizza podaci koje oracle ocekuje da dobijemo za prvih 10 zahtjeva iz test.json:\\n\\n\")  #konvertiramo predviđanja iz bool u integer\ndf_sample_submission = pd.read_csv('sampleSubmission.csv')\ndf_sample_submission.head()\n\ndf_output['requester_received_pizza'] = df_output['requester_received_pizza'].astype(int)   #ispišemo prvih 10 dobivenih elemenata\nprint(df_output.head(10))","metadata":{},"execution_count":15,"outputs":[{"name":"stdout","text":"requester_received_pizza podaci koje oracle ocekuje da dobijemo za prvih 10 zahtjeva iz test.json:\n\n\n  request_id  requester_received_pizza\n0   t3_i8iy4                         0\n1  t3_1mfqi0                         0\n2   t3_lclka                         0\n3  t3_1jdgdj                         0\n4   t3_t2qt4                         0\n5   t3_pvojb                         0\n6  t3_142n4c                         0\n7  t3_17rja6                         0\n8  t3_1lg6u2                         0\n9  t3_1b0mtx                         0\n","output_type":"stream"}]},{"cell_type":"code","source":"df_output.to_csv('basic-nlp-submission1.csv')  #kreiramo csv datoteku","metadata":{},"execution_count":null,"outputs":[]}]}